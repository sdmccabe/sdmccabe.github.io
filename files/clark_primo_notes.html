<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="generator" content="pandoc">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Notes on Clarke and Primo’s A Model Discipline</title>
  <style type="text/css">code{white-space: pre;}</style>
  <meta name="robots" content="noindex" />
  <link rel="stylesheet" href="../css/tufte.css">
  <link rel="stylesheet" href="../css/latex.css">
  <link rel="stylesheet" href="../css/pandoc.css">
  <link rel="stylesheet" href="../css/pandoc-solarized.css">
  <link rel="stylesheet" href="../css/tufte-extra.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header>
<h1 class="title">Notes on Clarke and Primo’s <em>A Model Discipline</em></h1>

<h2 class="author">Stefan McCabe</h2>
<p class="affilation"><em>Network Science Institute</em></p>

<h2 class="date">July 2018</h2>

</header>

<!--- ## Chapter 1 --->
<section id="overview" class="level2">
<h2>Overview</h2>
<p>Clarke and Primo (CP) advance a few central claims about political science in practice and its relationship to the philosophy of science. Primary among these is the <strong>model-based view</strong> that <em>models</em>—objects that are neither true nor false but are analogous to maps or metaphors—are the building blocks of science.</p>
<p>This may sound too airy and philosophical for political scientists, but, as CP note, existing practice <em>is already committed to a philosophical stance</em> and <em>that stance is wrong</em>.</p>
<!--- ## Chapter 2 --->
<p>From where do political scientists learn their philosophy of science? This is a primary role of graduate education; so they learn as students from exemplary works that speak to the scientific nature of the discipline; from textbooks, especially <a href="https://press.princeton.edu/titles/5458.html">King, Keohane, and Verba</a> (KKV), which is a staple of first-level graduate instruction; and from a few philosophers of science (mostly Popper) who are deemed “relevant.”</p>
<p>What do they learn from these works? What is the key “method” of political science? CP argue that it is <strong>hypothetico-deductivism</strong> (H-D), a method that can be decomposed into three familiar-looking parts:</p>
<ol type="1">
<li>propose a theory, then</li>
<li>derive predictions from the theory, and then</li>
<li>conduct falsification tests.</li>
</ol>
<p>Why is this the approach of political scientists? There are two answers:</p>
<blockquote>
<p>…this is how “real” scientists (chemists, physicists, biologists) go about their work. <em>The problem with this answer is that it is wrong</em> (21, emphasis mine).</p>
</blockquote>
<p>The other answer? Blame the logical positivists, William Riker, and <a href="http://eitminstitute.org/">EITM</a>. Riker was one of the few early political scientists to think deeply about the philosophy of science, and he was ultimately influenced by the logical positivists and by Popper. Graduate education and socialization, exemplified by programs like EITM, reinforce the ideas of Riker.</p>
<p>H-D is <a href="https://en.wikipedia.org/wiki/Falsifiability#Falsificationism">falsificationist</a>; a position most prominently associated with Popper. Fisher makes the sorts of statistical hypotheses considered by many social scientists amenable to falsificationist analysis through null hypothesis significance testing (NHST).</p>
<p>What are the problems with H-D (and also, therefore, NHST)? First, falsifiability is elusive. Most social-scientific hypotheses are <em>ceteris paribus</em> claims; these are non-falsifiable. We can learn “nothing that we did not know before” (35) from testing a model.<span><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle"/><span class="sidenote">This is reminiscent of the anti-emergence critiques of agent-based models.<br />
<br />
</span></span> The form of deductive argument used by H-D means that if the assumptions are true, the predictions are necessarily true—making the test uninformative—but if the assumption are false, the predictions could be either true or false, in which case we have also learned nothing from the test!</p>
<p>Even more problematically, social scientists routinely and intentionally use false assumptions (e.g., rational choice, unicameralism, etc.).</p>
<p>An additional problem for empirically-minded social scientists is that the sorts of data analysis required of social-scientific research mean that the data are never straightforward answers to a question; that is,</p>
<blockquote>
<p>Theories or models are never tested with data; they are tested with <em>models of data</em> (38).</p>
</blockquote>
<p>Furthermore, the <a href="https://en.wikipedia.org/wiki/Duhem%E2%80%93Quine_thesis">Quine-Duhem problem</a> says that “no theory is tested in isolation”:</p>
<blockquote>
<p>Testable predictions are derived from the hypothesis in question, supportive hypotheses, and <em>ceteris paribus</em> conditions. Therefore, it is impossible to falsify a theory or hypothesis because we cannot be sure whether the main hypothesis, one or more auxiliary hypotheses, a <em>ceteris paribus</em> clause, or a boundary condition is false (40).</p>
</blockquote>
<!--- ## Chapter 3 --->
<p>In place of H-D, CP advocate for <em>viewing models as maps</em>, and especially the <strong>Semantic Conception</strong> of models.</p>
<p>What does it mean to view a model as a map?</p>
<ol type="1">
<li>Models (and maps) are objects (neither true nor false).</li>
<li>Models contain inaccuracies; these inaccuracies do not impede their utility.</li>
<li>Models are necessarily incomplete; there are trade-offs as to what features of reality to represent.</li>
<li>Models are <em>purpose-relative</em>.</li>
</ol>
<p>What are some examples of these sorts of models in the literature? Krehbiel’s <a href="https://www.amazon.com/Pivotal-Politics-Theory-U-S-Lawmaking/dp/0226452727"><em>Pivotal Politics</em></a> model of gridlock is an excellent example; models with greater penetration into popular consciousness might include the Prisoner’s Dilemma or Schelling’s checkerboard model of segregation.</p>
<p>Models can explore the same system in different ways. For example, Krehbiel’s model and the well-known <a href="https://doi.org/10.2307/1961664">divide-the-dollar</a> and <a href="https://doi.org/10.2307/2082886">vote-buying</a> models are all models of legislative bargaining. These illustrative different aspects of the process and include different institutional features (e.g., the filibuster is crucial to Krehbiel’s model and is not present in the vote-buying model).</p>
<p>A model, consists of a “definition or stipulation” (70). This makes it non-falsifiable. Instead of inquiring about the truth of the model, we instead ask whether or not it is a useful map of real-world systems. This is the role of the <strong>theoretical hypothesis</strong>:</p>
<blockquote>
<p>Where a model defines a certain class of systems, a theoretical hypothesis asserts that certain (sorts of) real systems are among members of that class (71, citation omitted).</p>
</blockquote>
<p>This is a somewhat nebulous concept. What is similarity, after all? But the role of the theoretical hypothesis is to capture what we noted above: the purpose-relativity of the model.</p>
<p>What is the relationship between model and theory? In this framework, a <strong>theory</strong> is simply a set of models <span class="math inline">\(M_1, M_2, ... M_n\)</span>. These models each have their own theoretical hypothesis (or hypotheses) connecting the model to a feature (or features) <span class="math inline">\(f_1, f_2, ... f_m\)</span> of a relevant real-world system.</p>
<p>A key implication of this line of reasoning is that <em>there is no one true model</em>.</p>
<!--- ## Chapter 4 --->
</section>
<section id="varieties-of-models" class="level2">
<h2>Varieties of Models</h2>
<p>CP divide models into two categories, theoretical and empirical. All of the examples used thus far have been theoretical models; indeed, as we will see, CP are not big fans of empirical models.</p>
<p>There are four important features of <strong>theoretical models</strong>. The first is their <em>purpose-relativity</em>, a property we have already noted. The others are their use of <em>deductive reasoning</em>, the <em>techniques</em> used in their construction,<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">The focus in the book is on social-choice-inspired or game-theoretic models, but it is not clear why. They specifically note that their arguments apply to verbal models and to computational (agent-based) models as well, but use few examples to illustrate the utility of such models.<br />
<br />
</span></span> and their <em>level of abstraction</em>.</p>
<p>In addition to the theoretical/empirical divide, CP argue that models can be classified by purpose. These purposes differ across the theoretical/empirical divide; for theoretical models these purposes are:</p>
<ul>
<li><strong>foundational</strong>: a model that is adaptable to many scenarios and suitable for inspiring other models. Arrow’s Theorem, divide-the-dollar, and Putnam’s two-level game are all exemplars.</li>
<li><strong>organizational</strong>: a model that collects and organizes a number of existing empirical features of a system.<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">While CP are applying this typology in terms of <em>models</em>, this purpose seems much better suited to a <em>theory</em>. Theories will be discussed later in the book.<br />
<br />
</span></span> Achen’s model of prospective voting and party identification is an example, as is Fenno’s work on home styles.</li>
<li><strong>exploratory</strong>: a model for investigating causal mechanisms, like the Romer-Rosenthal <a href="https://doi.org/10.1007/BF03187594">agenda-setting</a> model or the analytic narratives movement.</li>
<li><strong>predictive</strong>: self-explanatory but not particularly relevant for theoretical models.</li>
</ul>
<p>The critical claim here is that <em>prediction is usually not the right standard by which to judge theoretical models</em>.<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">This discussion is quite reminiscent of the the debate in <em>JASSS</em> between <a href="http://jasss.soc.surrey.ac.uk/11/4/12.html">Joshua Epstein</a> and <a href="http://jasss.soc.surrey.ac.uk/12/1/9.html">Nicholas Thompson</a>.<br />
<br />
</span></span></p>
<p>The claim is surprising and counter-intuitive to political scientists, but actual practice shows that they shouldn’t be. Spatial models have <em>terrible</em> predictive accuracy but have proven both enormously popular among political scientists and enormously fruitful as a research agenda. Smaller examples abound as well. Consider Krehbiel’s <em>Pivotal Politics</em> model again: it is well respected for its utility in modeling legislative gridlock <em>despite predicting that vetoes and filibusters will never occur!</em></p>
<p>Why is it in the interest of modelers to accept this proposition? After all, even if CP are right, from a sociological perspective it might still be in the best interest of political scientists to pay lip service to prediction and to falsifiability.</p>
<p>First, defending the falsifiability of a model is a <em>distraction</em>; attempting to shore up a model from attack on grounds of predictive accuracy means foregoing the freedom to explore within the model. Second, it <em>provides a cudgel</em> to critics, best exemplified by the critics of rational choice theory:</p>
<blockquote>
<p>Much of Green and Shapiro’s critique of rational choice models [in <a href="https://www.amazon.com/Pathologies-Rational-Choice-Theory-Applications/dp/0300066368"><em>Pathologies of Rational Choice Theory</em></a>] hinges on prediction and a falsificationist or perhaps Lakatosian view of the world, their protestations to the contrary…. Green and Shapiro, like many others, mistake a lack of predictive accuracy for a lack of usefulness. To the extent that scholars refer to their models as superior or better because they are “testable” or “falsifiable,” and to the extent that the EITM movement argues that testability is the primary metric by which to judge theoretical models, Green and Shapiro seem to have defined the terms of the debate. This outcome is, to say the least, unfortunate (97, citations omitted).</p>
</blockquote>
<p>Once we move away from predictive accuracy as the <em>sine qua non</em> of good modeling, we need new criteria for model evaluation. CP propose two dimensions for evaluating model quality: <em>fecundity</em>, or the number of insights produced; and the <em>importance</em> of the insights.</p>
<p>In general, they are hesitant to provide what they call “cookbook” answers for creating a good/bad model dichotomy.<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">Given social scientists’ abuse of dichotomies <a href="https://doi.org/10.1080/01621459.2017.1289846">in other areas</a>, this is probably a healthy approach.<br />
<br />
</span></span> Instead, the evaluation of theoretical model quality is the domain of political scientists as a group, through peer review and other traditional mechanisms of collective decision-making.</p>
<!--- ## Chapter 5 --->
<p>In contrast to abstract and game-theoretic theoretical models, <strong>empirical models</strong> are statistical in nature. CP’s preferred definition of a statistical model breaks the concept into three components (the components of a linear regression are included in parentheses to illustrate):</p>
<ol type="1">
<li>a probability model (<span class="math inline">\(Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\)</span>)</li>
<li>a sampling model (independent samples from an infinite population)</li>
<li>a statistical generating mechanism (the error term is normally distributed)</li>
</ol>
<p>That is, “a statistical model is not simply a theoretical model with an appended error term” (108).</p>
<p>Why insist on the distinction between theoretical and empirical models? Aren’t they all just models, and therefore have the same goals? In a way, this is true; many of the same points discussed for theoretical models apply to empirical models as well. However, there is a crucial additional constraint on empirical models.</p>
<!--
> The key difference is that an empirical model, regardless of its other uses,
> should describe accurately the dependencies within a given data set. Empirical
> models therefore cannot attain the same level of generality that theoretical
> models do because where theory is general, data are specific, tied to
> particular places and times (105). 
-->
<p>The contingency of data sets means that researchers do not have the same freedom to model as they do in the theoretical context. It also means that, whereas assumptions in theoretical models are free to be inaccurate or even intentionally wrong, the assumptions of empirical models must be rigorously checked. However, empirical models are still purpose-relative and neither true nor false, just like theoretical models.</p>
<p>We have discussed the uses of theoretical models already; the purposes of empirical models are:</p>
<ol type="1">
<li><strong>theory testing</strong>: the H-D-influenced interrogation of a theoretical model with data.</li>
<li><strong>forecasting</strong>: that is, real prediction (not postdiction or comparative statics).</li>
<li><strong>measurement</strong>: summarizing the “relevant available knowledge” (113) about a feature of interest.</li>
<li><strong>characterization</strong>: the identification of “relationships within a data set” (114).<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">Cyrus Samii’s recent blog post on <a href="http://cyrussamii.com/?p=2687">descriptive quantitative work</a> seems similar to this category.<br />
<br />
</span></span></li>
</ol>
<p>This typology seems obvious but masks what might now be a highly controversial claim:</p>
<blockquote>
<p>The point of these examples is that an empirical model can be useful without reliance on the more controversial aspects of empirical analysis: statistical and causal inference… (114).</p>
</blockquote>
<p>It should be abundantly clear by this point that CP are skeptical about the utility of theory testing. Regardless of the rhetoric used to justify theory testing—CP identify the already-discussed falsificationism, its moderate sibling verificationism, and Bayesian confirmation<span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">While I am not familiar with the literature on Bayesian confirmation, the rather cursory treatment of this approach in the chapter, coupled with the paucity of citations, makes me wonder if this argument is sufficiently charitable toward the Bayesians.<br />
<br />
</span></span> as approaches—the theory tester is always guilty of the fallacy of affirming the consequent.</p>
<p>Luckily, there are well-known examples of empirical work in political science that are not motivated by theory testing. For example, Poole-Rosenthal <a href="https://www.amazon.com/Ideology-Congress-Keith-T-Poole/dp/1412806089">NOMINATE</a> scores are designed for predictive accuracy, not to evaluate some prespecified theoretical model.<span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote">I find the examples used to be somewhat muddy, and evidently so do CP. Poole-Rosenthal is used as an example of prediction, but most people think of NOMINATE as providing measurement. Indeed, good measurements should have predictive accuracy! As CP note, “[t]hat these categories are fluid in no way diminishes that these papers are exemplars of science and yet sport none of the features that political scientists have come to regard as scientific” (130).<br />
<br />
</span></span></p>
<!--- ## Chapter 6 --->
</section>
<section id="using-models-to-explain" class="level2">
<h2>Using Models to Explain</h2>
<p>Having considered empirical and theoretical models each in isolation, it is now time to ask: what is the relationship between the two?</p>
<blockquote>
<p>Our claim in this chapter is that theoretical models and empirical models interact in the same ways that different maps do. That is, a theoretical model can often be used to help identify an empirical model. Empirical models, on the other hand, can often be used to clarify the details of a theoretical model (137).</p>
</blockquote>
<p>This is not the same relationship as the traditional approach, in which empirical models are used to “test” theoretical ones. Nor is it one in which theoretical and empirical models are on equal footing; theoretical models have primacy:</p>
<blockquote>
<p>Theoretical models can be used to explain findings or generalization produced by empirical models. Empirical models, on the other hand, cannot provide explanations. The need to explain empirical findings provides a strong justification for combining theoretical and empirical models (137).</p>
</blockquote>
<p>What does it mean to say that an empirical model “cannot provide explanations?” Thus far, we have focused on models as maps, with little recourse to the language of explanation, even though explanation is the “single overarching goal that serves to unite the disparate purposes of social scientific models” (140).</p>
<p>The logical positivists struggled with explanation, and while their approach—the <strong>deductive-nomological</strong> or <strong>covering law</strong> model—is well known, it is largely outdated. It held that explanations are logical arguments that develop general laws such that events can be explained deductively in terms of these general laws. (There is also a modification called the <strong>inductive-statistical</strong> account that adapts this theory to statistical arguments.) A successor theory that is more robust to criticism is what Clarke and Primo call the <strong>unification conception</strong>:</p>
<blockquote>
<p>Although the details of the unificationist account have become quite complex, the basic idea is simple: scientific explanation consists of unifying a number of empirical regularities with a minimum number of theoretical concepts or assumptions (144).</p>
</blockquote>
<p>The alternative approach is the <strong>causal-mechanical</strong> account of explanation, in which explanations establish causal links.<span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="sidenote">The account of causal-mechanical explanation is also quite unsatisfactory to me. It focuses primarily on the work of <a href="https://en.wikipedia.org/wiki/Wesley_C._Salmon">Wesley Salmon</a>. His model is interesting, to be sure, but CP note that Salmon himself had largely abandoned it. While the two have different approaches, <a href="https://en.wikipedia.org/wiki/Judea_Pearl">Judea Pearl</a> also provides a theory of explanation that emphasizes causal mechanisms and Pearl is someone <em>social scientists actually read!</em><br />
<br />
</span></span> This approach might be more familiar to readers.</p>
<p>These accounts exist largely outside of political science; within the discipline, KKV “argue that that to propose an explanation is to propose a cause” (146), and causal language is typically counterfactual. However, those influenced by game theory tend to conceptions closer to the unification view.</p>
<p>What has all the discussion of explanation to do with models? CP profess agnosticism as to unification or causal-mechanical explanation, but make two provocative claims: first, that “the identification of a cause in and of itself is an insufficient basis for an explanation” (153); and second, that truth is not “a necessary condition for an explanation” (153). Both are apparently provocative claims that we tend to accept in practice; for example in the case of Newton’s (not strictly true) laws.</p>
<p>Just as provocative is the claim that empirical models cannot be explanations. To illustrate, consider the well-known regression model studying the effect of shark attacks on Woodrow Wilson’s vote share, recently illustrated in Achen and Bartels’s <a href="https://www.amazon.com/Democracy-Realists-Elections-Responsive-Government/dp/0691178240"><em>Democracy for Realists</em></a>.<span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="sidenote">While it doesn’t matter for the purpose of illustration, it is worth remembering that <a href="http://www.andrewbenjaminhall.com/FowlerHall_Sharks.pdf">this example is wrong and people should stop using it</a>.<br />
<br />
</span></span> The regression does not speak for itself; that is, one can imagine the same model being used to demonstrate that Wilson’s 1916 vote share is predicted by his 2012 vote share, that political machines disliked Wilson, or that voters inappropriately punished Wilson for events outside his control.<span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="sidenote">It is not at all obvious why a regression model should speak for itself or that it should be possible “to discern a single explanation from simply looking at an empirical model” (155).<br />
<br />
</span></span></p>
<p>CP prefer to use theoretical models, especially of the organizational variety, to account for empirical regularities rather than allowing the data to speak for itself. This raises the possibility of two theoretical models organizing the same empirical regularities. What to do? In the case of empirical models, there are some approaches to comparative model testing, like Vuong’s likelihood ratio test, but comparing theoretical models is much more difficult. Furthermore, comparative model testing can only ever be <em>relative</em>; it can never speak to model quality in general terms.</p>
<p>But why test theoretical models against one another at all? Theoretical models need not be mutually exclusive:</p>
<blockquote>
<p>Consider the 2009 crash of Colgan Air Flight 3407 outside of Buffalo, New York. Explanations of the crash include human error (the captain did not react properly to an imminent stall), lax regulation (the captain failed for Federal Aviation Administration check flights, both pilots were fatigued), the weather (the captain, fearing icing, may have overreacted after an icing video distributed by the airline), unrealistic training (the use of simulators that fail to mimic in-flight conditions), equipment (the airspeed indicator lacked low-speed awareness features), and inadequate airline standard operation procedures…. Essentially, all of these explanations are both simultaneously correct and simultaneously wrong. Choosing between them is unnecessary… (164–165).</p>
</blockquote>
<!--- ## Chapter 7 --->
</section>
<section id="addressing-counterarguments" class="level2">
<h2>Addressing Counterarguments</h2>
<p>Clarke and Primo conclude by preempting a number of counterarguments:</p>
<ol type="1">
<li>This is a distraction. (It’s not; it speaks to everyday work in political science, like referee reports.)</li>
<li>I don’t want to be criticized for holding this position. (Read the book.)</li>
<li>Maps are tested. (Maps are tested for suitability for a purpose, which is what they advocate.)</li>
<li>Model evaluation is imprecise. (Yes, and that’s a good thing.)</li>
<li>While EITM cannot claim to have advanced the field, neither can Clarke and Primo. (Whatever.)</li>
<li>This excludes qualitative and computational models. (Everything they say applies for those models; see also <a href="http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195392753.001.0001/oxfordhb-9780195392753-e-15">Kollman</a>.)</li>
<li>What should political scientists be doing differently? (Teach scope and methods classes differently.)</li>
<li>Where do we go from here? (Forward!)</li>
</ol>
<!--- Footnotes --->
<!--- Links --->

<h2>References</h2>
<div id="refs" class="references">
<div id="ref-druckman_unmet_2009">
<p>Druckman, James N., James H. Kuklinski, and Lee Sigelman. 2009. “The Unmet Potential of Interdisciplinary Research: Political Psychological Approaches to Voting and Public Opinion.” <em>Political Behavior</em> 31 (4): 485–510. <a href="https://doi.org/10.1007/s11109-009-9092-2" class="uri">https://doi.org/10.1007/s11109-009-9092-2</a>.</p>
</div>
</div>
</section>

</article>
</body>
</html>
